{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "This notebook walks through the current RAG retrieval method used on an Obsidian notes directory containing knowledge on growing Cannabis in Living Soil.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Markdown Files\n",
    "Using Obsidian notes provides several advantages:\n",
    "- Notes are easy to write and edit.\n",
    "- Frontmatter/tags transfer into the metadata of the nodes.\n",
    "- The Headers provide natural splitting points for the text.\n",
    "\n",
    "The `IngestService` class relies on Langchain's `ObsidianLoader` class to load the notes.  The `ObsidianLoader` class uses the frontmatter, tags, dataview fields, and file metadata to populate the metadata of the nodes.  The additional metadata will benefit retrieval of the best nodes to answer a given query.\n",
    "\n",
    "The `load_obsidian_notes` method takes in either:\n",
    "- a list of strings where each string is considered a markdown file.\n",
    "- a directory path to an Obsidian vault.\n",
    "The list of strings options is useful for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of an obsidian note:\n",
    "doc = \"\"\"#Calcium_additive #raise_ph #Wollastonite #Silicon_additive #buffer_pH #Calcium\n",
    "Growers  turn to Wollastonite for:\n",
    "- Its **liming** capability.  Wollastonite's dissolution rate is slower than agricultural lime, offering a buffering effect against rapid pH changes. This makes Wollastonite beneficial in areas with fluctuating acidity levels.\n",
    "- Adding **Silicon**.\n",
    "- Adding **Calcium**.\n",
    "Wollastonite's pH buffering effect and Silicon content contribute to pest control and powdery mildew suppression, although the exact mechanisms are not fully understood.\n",
    "\n",
    "# What is Wollastonite?\n",
    "\n",
    "## Formation\n",
    "Wollastonite is formed when Limestone is subjected to heat and pressure during metamorphism if surrounding silicate minerals are present.\n",
    "### Basic Reaction:\n",
    "Given high pressure and high temperature:\n",
    "- CaCO3 (Limestone) + SiO2 (silica) → CaSiO3 (Wollastonite) + CO2 (carbon Dioxide)\n",
    "## Sources\n",
    "China is the largest producer of Wollastonite. Other areas where Wollastonite is mined include the United States (although it was originally mined in California, the only active mining in the U.S. is now in New York State), India, Mexico, Canada, and Finland.\n",
    "\n",
    "## Industrial Applications of Wollastonite\n",
    "\n",
    "|Industry|Application|\n",
    "|---|---|\n",
    "|Ceramics|Smoother and more durable ceramics, reinforcement agent|\n",
    "|Plastics and Rubber|Cost-effective strengthening agent|\n",
    "|Paints and Coatings|Reinforcement, improved durability and impact resistance|\n",
    "|Construction|Improved strength and durability of building materials, safe alternative to asbestos|\n",
    "##  How Wollastonite Provides Plants with Ca and Si\n",
    "\n",
    "Wollastonite reacts with Water and Carbon Dioxide in the soil to form Calcium Bicarbonate and Silicon Dioxide.\n",
    "- CaSiO₃ (Wollastonite)+2CO₂ (carbon Dioxide,)+H₂O (Water)→Ca(HCO₃)₂ (Calcium bicarbonate)+SiO₂ (silica)\n",
    "\n",
    "### Calcium\n",
    "- Calcium bicarbonate  (Ca(HCO₃)₂) is unstable and fairly easily decomposes to Limestone (CaCO₃):\n",
    "\t\t- Ca(HCO₃)₂ (Calcium bicarbonate)→CaCO₃ (Limestone)+  CO₂ (carbon Dioxide) + H₂O (Water)\n",
    "\n",
    "- Soils with a pH below 7 (acidic soils) contain hydrogen ions (H+). These hydrogen ions react with the Limestone (CaCO3) to form Calcium ions (Ca2+), Water (H2O), and Carbon Dioxide (CO2).\n",
    "\t- CaCO3 (Limestone) + 2H+ (hydrogen ions) → Ca2+ (Calcium ions) + H2O (Water) + CO2 (carbon Dioxide)\n",
    "### Silicon\n",
    "- Silicon Dioxide slowly breaks down into Silicic Acid, which plants absorb. This process is influenced by soil pH, temperature, and microbial activity.\n",
    "\t- SiO2 (Silicon Dioxide) + 2H2O (Water) → H4SiO4 (Silicic Acid)\n",
    "\n",
    "- Plants absorb Silicic Acid from the soil solution through their roots.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# --->: Read in the markdown files in the Obsidian vault directory\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mingest_service\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IngestService\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# The Directory containing the knowledge documents used by the AI to do the analysis on the soil tests.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# soil_knowledge_directory = r\"G:\\My Drive\\Audios_To_Knowledge\\knowledge\\AskGrowBuddy\\AskGrowBuddy\\Knowledge\\soil_test_knowlege\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Load the documents\u001b[39;00m\n\u001b[0;32m      6\u001b[0m ingest_service \u001b[38;5;241m=\u001b[39m IngestService()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "# --->: Read in the markdown files in the Obsidian vault directory\n",
    "from src.ingest_service import IngestService\n",
    "# The Directory containing the knowledge documents used by the AI to do the analysis on the soil tests.\n",
    "# soil_knowledge_directory = r\"G:\\My Drive\\Audios_To_Knowledge\\knowledge\\AskGrowBuddy\\AskGrowBuddy\\Knowledge\\soil_test_knowlege\"\n",
    "# Load the documents\n",
    "ingest_service = IngestService()\n",
    "# loaded_documents = ingest_service.load_obsidian_notes(soil_knowledge_directory)\n",
    "loaded_documents = ingest_service.load_obsidian_notes([doc])\n",
    "# Show some summary stats about the documents\n",
    "from src.doc_stats import DocStats\n",
    "DocStats.print_llama_index_docs_summary_stats(loaded_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the Obsidian notes into chunks\n",
    "LlamaIndex's `MarkdownNodeParser` class is used to split the documents into chunks.  This allows for natural splitting of the text using the headers in the markdown files.     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --->: Chunk the documents\n",
    "# LlamaIndex likes to call these nodes.\n",
    "# limited_docs = loaded_documents[:2]\n",
    "limited_docs = loaded_documents\n",
    "nodes = ingest_service.chunk_text(limited_docs)\n",
    "print(f\"Number of documents used for chunking: {len(limited_docs)}\")\n",
    "print(f\"Number of nodes created: {len(nodes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up Ollama\n",
    "We are using Ollama to provide both the embedding model and the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --->: Set up the local embedding model and LLM\n",
    "# Set embedding model\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "Settings.embed_model = OllamaEmbedding(\n",
    "    model_name='nomic-embed-text',\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    ollama_additional_kwargs={\"mirostat\": 0},\n",
    ")\n",
    "# Choose your LLM...\n",
    "Settings.llm = Ollama(model='mistral', request_timeout=1000.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up Multi-Index Fusion Retrieval\n",
    "RAG will use three index/retrieval methods:\n",
    "1. Vector Store Index. This provides a similarity search on vector embeddings of the nodes.\n",
    "2. BM25 Index. This offers keyword-based retrieval, ranking documents based on term frequency and inverse document frequency.\n",
    "3. Knowledge Graph Index. This captures relationships between entities and concepts, enabling context-aware and relationship-based retrieval.\n",
    "\n",
    "The retrieved nodes are then passed through a Cohere's Reranker to rerank the nodes.  \n",
    "\n",
    "By using a combination of these methods, we create a retrieval system that integrates semantic similarity, keyword relevance, and relational context. This approach aims to improve the likelihood of retrieving relevant information compared to using vector similarity search alone.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Persist the Vector Store Index\n",
    "Chroma is used to store and query the vector embeddings of the nodes.  Ollama embeddings are used to vectorize the text.\n",
    "## Simple Testing       \n",
    "For simple testing, the overhead of using a Chroma db can be avoided.  This code proved useful:\n",
    "```\n",
    "from llama_index.core import VectorStoreIndex\n",
    "vector_index = VectorStoreIndex(nodes)\n",
    "cache_dir = \"./vector_index_cache\"\n",
    "vector_index.storage_context.persist(persist_dir=cache_dir)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ingest_service = IngestService()\n",
    "# Create a Chroma collection object of a given name. Metadata, embeddings, text are all added.\n",
    "our_collection = ingest_service.create_collection(docs=nodes, collection='test', embedding_model='nomic-embed-text')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from src.ingest_service import IngestService\n",
    "# Grab the vector index\n",
    "ingest_service = IngestService()\n",
    "our_collection = ingest_service.get_collection('test')\n",
    "chroma_vector_store = ChromaVectorStore(chroma_collection=our_collection)\n",
    "# Create a VectorStoreIndex using the ChromaVectorStore\n",
    "vector_index = VectorStoreIndex.from_vector_store(chroma_vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Persist a Knowledge Graph Index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The procedure has a deprecated field. ('config' used by 'apoc.meta.graphSample' is deprecated.)} {position: line: 1, column: 1, offset: 0} for query: \"CALL apoc.meta.graphSample() YIELD nodes, relationships RETURN nodes, [rel in relationships | {name:apoc.any.property(rel, 'type'), count: apoc.any.property(rel, 'count')}] AS relationships\"\n"
     ]
    }
   ],
   "source": [
    "from llama_index.graph_stores.neo4j import Neo4jPropertyGraphStore\n",
    "neo4j_uri = \"bolt://localhost:7687\"  # Update with your Neo4j instance details\n",
    "neo4j_user = \"neo4j\"\n",
    "neo4j_password = \"asd@123qwe\"\n",
    "graph_store = Neo4jPropertyGraphStore(\n",
    "    username=neo4j_user,\n",
    "    password=neo4j_password,\n",
    "    url=neo4j_uri,\n",
    "    database=\"test\",  # Use appropriate database name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_index = ProeprtyGraphIndex.from_documents(nodes,embed_model=Settings.embed_model, kg_extractors=[SchemaLLMPathExtractor(llm=Settings.llm, temperature=0.01)],property_graph=graph_store,\n",
    "show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import  KnowledgeGraphIndex\n",
    "kg_index = KnowledgeGraphIndex(nodes, graph_store=graph_store,max_triplets_per_chunk=2,show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   for node_id, node in kg_index.docstore.docs.items():\n",
    "       print(f\"Node ID: {node_id}\")\n",
    "       print(f\"Node content: {node.get_content()}\")\n",
    "       print(f\"Node metadata: {node.metadata}\")\n",
    "       print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import  PropertyGraphIndex\n",
    "from llama_index.graph_stores.neo4j import Neo4jPropertyGraphStore\n",
    "# Define Neo4j connection credentials\n",
    "neo4j_uri = \"bolt://localhost:7687\"  # Update with your Neo4j instance details\n",
    "neo4j_user = \"neo4j\"\n",
    "neo4j_password = \"asd@123qwe\"\n",
    "# neo4j_password = os.getenv(\"NEO4J_PASSWORD\")  # Securely use an environment variable\n",
    "\n",
    "# Instantiate Neo4jGraphStore\n",
    "graph_store = Neo4jPropertyGraphStore(\n",
    "    username=neo4j_user,\n",
    "    password=neo4j_password,\n",
    "    url=neo4j_uri,\n",
    "    database=\"test\",  # Use appropriate database name\n",
    ")\n",
    "\n",
    "# According to LlamaIndex, I should be using PropertyGraphIndex instead of KnowledgeGraphIndex. However, it is not possible\n",
    "# because PropertyGraphIndex throws the exception: AttributeError: 'Neo4jGraphStore' object has no attribute 'supports_vector_queries'\n",
    "# Create Knowledge Graph Index from documents\n",
    "# from llama_index.core import StorageContext\n",
    "# storage_context = StorageContext.from_defaults(graph_store=graph_store)\n",
    "from llama_index.core import load_index_from_storage\n",
    "storage_context = StorageContext.from_defaults(graph_store=graph_store, persist_dir=\"graph_store\")\n",
    "# Try to load the index, if it doesn't exist, create it\n",
    "\n",
    "try:\n",
    "    kg_index = load_index_from_storage(storage_context)\n",
    "    print(\"Loaded existing index\")\n",
    "except ValueError:\n",
    "    print(\"Creating new index\")\n",
    "    # Assuming 'nodes' is your list of document nodes\n",
    "    kg_index = PropertyGraphIndex(\n",
    "        nodes,\n",
    "        max_triplets_per_chunk=2,\n",
    "        show_progress=True,\n",
    "        storage_context=storage_context\n",
    "    )\n",
    "    persist_dir = \"graph_store\"\n",
    "    kg_index.storage_context.persist(persist_dir=persist_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_index.storage_context.persist(persist_dir=persist_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_index.as_query_engine(\"What is Wollastonite?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   query_engine = kg_index.as_query_engine()\n",
    "   response = query_engine.query(\"What are the industrial applications of Wollastonite?\")\n",
    "   print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing knowledge graph.\n",
    "from llama_index.graph_stores.neo4j import Neo4jPropertyGraphStore\n",
    "from llama_index.core import PropertyGraphIndex\n",
    "from llama_index.core.storage import StorageContext\n",
    "\n",
    "neo4j_uri = \"bolt://localhost:7687\"  # Update with your Neo4j instance details\n",
    "neo4j_user = \"neo4j\"\n",
    "neo4j_password = \"asd@123qwe\"\n",
    "\n",
    "graph_store = Neo4jPropertyGraphStore(\n",
    "    username=neo4j_user,\n",
    "    password=neo4j_password,\n",
    "    url=neo4j_uri,\n",
    "    database=\"test\",  # Use appropriate database name\n",
    ")\n",
    "# Create a StorageContext with your graph_store\n",
    "storage_context = StorageContext.from_defaults(property_graph_store=graph_store)\n",
    "\n",
    "kg_index = PropertyGraphIndex.from_existing(\n",
    "    property_graph_store=graph_store,\n",
    "    storage_context=storage_context,\n",
    "    llm=Settings.llm,\n",
    "    embed_model=Settings.embed_model,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'charmap' codec can't encode character '\\u2192' in position 17032: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[90], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m response \u001b[38;5;241m=\u001b[39m query_engine\u001b[38;5;241m.\u001b[39mquery(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<QUERY>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# save and load\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpersist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpersist_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./storage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StorageContext, load_index_from_storage\n\u001b[0;32m     26\u001b[0m index \u001b[38;5;241m=\u001b[39m load_index_from_storage(\n\u001b[0;32m     27\u001b[0m     StorageContext\u001b[38;5;241m.\u001b[39mfrom_defaults(persist_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./storage\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\happy\\Documents\\Projects\\askgrowbuddy\\.venv\\Lib\\site-packages\\llama_index\\core\\storage\\storage_context.py:183\u001b[0m, in \u001b[0;36mStorageContext.persist\u001b[1;34m(self, persist_dir, docstore_fname, index_store_fname, vector_store_fname, image_store_fname, graph_store_fname, pg_graph_store_fname, fs)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph_store\u001b[38;5;241m.\u001b[39mpersist(persist_path\u001b[38;5;241m=\u001b[39mgraph_store_path, fs\u001b[38;5;241m=\u001b[39mfs)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproperty_graph_store:\n\u001b[1;32m--> 183\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproperty_graph_store\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpersist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpersist_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpg_graph_store_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;66;03m# save each vector store under it's namespace\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m vector_store_name, vector_store \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvector_stores\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32mc:\\Users\\happy\\Documents\\Projects\\askgrowbuddy\\.venv\\Lib\\site-packages\\llama_index\\core\\graph_stores\\simple_labelled.py:169\u001b[0m, in \u001b[0;36mSimplePropertyGraphStore.persist\u001b[1;34m(self, persist_path, fs)\u001b[0m\n\u001b[0;32m    167\u001b[0m     fs \u001b[38;5;241m=\u001b[39m fsspec\u001b[38;5;241m.\u001b[39mfilesystem(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mopen(persist_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m--> 169\u001b[0m     \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_dump_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.12.0\\Lib\\encodings\\cp1252.py:19\u001b[0m, in \u001b[0;36mIncrementalEncoder.encode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharmap_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43mencoding_table\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'charmap' codec can't encode character '\\u2192' in position 17032: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "from llama_index.core import PropertyGraphIndex\n",
    "\n",
    "# create\n",
    "index = PropertyGraphIndex.from_documents(\n",
    "    loaded_documents,\n",
    ")\n",
    "\n",
    "# use\n",
    "retriever = index.as_retriever(\n",
    "    include_text=True,  # include source chunk with matching paths\n",
    "    similarity_top_k=2,  # top k for vector kg node retrieval\n",
    ")\n",
    "nodes = retriever.retrieve(\"<QUERY>\")\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    include_text=True,  # include source chunk with matching paths\n",
    "    similarity_top_k=2,  # top k for vector kg node retrieval\n",
    ")\n",
    "response = query_engine.query(\"what is wollastonite?\")\n",
    "\n",
    "# save and load\n",
    "index.storage_context.persist(persist_dir=\"./storage\")\n",
    "\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "\n",
    "index = load_index_from_storage(\n",
    "    StorageContext.from_defaults(persist_dir=\"./storage\")\n",
    ")\n",
    "\n",
    "# loading from existing graph store (and optional vector store)\n",
    "# load from existing graph/vector store\n",
    "index = PropertyGraphIndex.from_existing(\n",
    "    property_graph_store=graph_store, vector_store=vector_store, llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type set is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[94], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# save and load\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpersist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpersist_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgraph_store\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\happy\\Documents\\Projects\\askgrowbuddy\\.venv\\Lib\\site-packages\\llama_index\\core\\storage\\storage_context.py:183\u001b[0m, in \u001b[0;36mStorageContext.persist\u001b[1;34m(self, persist_dir, docstore_fname, index_store_fname, vector_store_fname, image_store_fname, graph_store_fname, pg_graph_store_fname, fs)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph_store\u001b[38;5;241m.\u001b[39mpersist(persist_path\u001b[38;5;241m=\u001b[39mgraph_store_path, fs\u001b[38;5;241m=\u001b[39mfs)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproperty_graph_store:\n\u001b[1;32m--> 183\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproperty_graph_store\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpersist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpersist_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpg_graph_store_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;66;03m# save each vector store under it's namespace\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m vector_store_name, vector_store \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvector_stores\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[1;32mIn[93], line 10\u001b[0m, in \u001b[0;36mcustom_persist\u001b[1;34m(self, persist_path, fs)\u001b[0m\n\u001b[0;32m      8\u001b[0m     fs \u001b[38;5;241m=\u001b[39m fsspec\u001b[38;5;241m.\u001b[39mfilesystem(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mopen(persist_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 10\u001b[0m     \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.12.0\\Lib\\json\\__init__.py:179\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    173\u001b[0m     iterable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(skipkeys\u001b[38;5;241m=\u001b[39mskipkeys, ensure_ascii\u001b[38;5;241m=\u001b[39mensure_ascii,\n\u001b[0;32m    174\u001b[0m         check_circular\u001b[38;5;241m=\u001b[39mcheck_circular, allow_nan\u001b[38;5;241m=\u001b[39mallow_nan, indent\u001b[38;5;241m=\u001b[39mindent,\n\u001b[0;32m    175\u001b[0m         separators\u001b[38;5;241m=\u001b[39mseparators,\n\u001b[0;32m    176\u001b[0m         default\u001b[38;5;241m=\u001b[39mdefault, sort_keys\u001b[38;5;241m=\u001b[39msort_keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\u001b[38;5;241m.\u001b[39miterencode(obj)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;66;03m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;66;03m# a debuggability cost\u001b[39;00m\n\u001b[1;32m--> 179\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.12.0\\Lib\\json\\encoder.py:432\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[1;34m(o, _current_indent_level)\u001b[0m\n\u001b[0;32m    430\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[0;32m    431\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m--> 432\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.12.0\\Lib\\json\\encoder.py:406\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[1;34m(dct, _current_indent_level)\u001b[0m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    405\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[1;32m--> 406\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    408\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.12.0\\Lib\\json\\encoder.py:439\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[1;34m(o, _current_indent_level)\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCircular reference detected\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    438\u001b[0m     markers[markerid] \u001b[38;5;241m=\u001b[39m o\n\u001b[1;32m--> 439\u001b[0m o \u001b[38;5;241m=\u001b[39m \u001b[43m_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m _iterencode(o, _current_indent_level)\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.12.0\\Lib\\json\\encoder.py:180\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[0;32m    162\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;124;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    178\u001b[0m \n\u001b[0;32m    179\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 180\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    181\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis not JSON serializable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Object of type set is not JSON serializable"
     ]
    }
   ],
   "source": [
    "# save and load\n",
    "index.storage_context.persist(persist_dir=\"graph_store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Any\n",
    "from llama_index.core.graph_stores.simple_labelled import SimplePropertyGraphStore\n",
    "\n",
    "def custom_persist(self, persist_path: str, fs: Any = None) -> None:\n",
    "    if fs is None:\n",
    "        import fsspec\n",
    "        fs = fsspec.filesystem(\"file\")\n",
    "    with fs.open(persist_path, \"w\", encoding='utf-8') as f:\n",
    "        json.dump(self.graph.model_dump(), f, ensure_ascii=False)\n",
    "\n",
    "# Monkey-patch the persist method\n",
    "SimplePropertyGraphStore.persist = custom_persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_property_graph_index(kg_index):\n",
    "    print(\"Inspecting PropertyGraphIndex\\n\")\n",
    "\n",
    "    # 1. Check the property graph store\n",
    "    print(f\"Property graph store type: {type(kg_index.property_graph_store)}\")\n",
    "\n",
    "    # 2. Get all nodes\n",
    "    all_nodes = kg_index.property_graph_store.get()\n",
    "    print(f\"\\nNumber of nodes: {len(all_nodes)}\")\n",
    "    print(\"Sample nodes:\", all_nodes[:5] if len(all_nodes) > 5 else all_nodes)\n",
    "\n",
    "    # 3. Inspect a specific node\n",
    "    if all_nodes:\n",
    "        sample_node = all_nodes[0]\n",
    "        print(f\"\\nSample node:\")\n",
    "        print(f\"  ID: {sample_node.id}\")\n",
    "        print(f\"  Labels: {sample_node.labels}\")\n",
    "        print(f\"  Properties: {sample_node.properties}\")\n",
    "\n",
    "        # 4. Get relationships for the sample node\n",
    "        relationships = kg_index.property_graph_store.get_node_relationships(sample_node.id)\n",
    "        print(f\"\\nRelationships for sample node:\")\n",
    "        for rel in relationships:\n",
    "            print(f\"  {rel['source']} -{rel['relation']}-> {rel['target']}\")\n",
    "\n",
    "    # 5. Perform a query using the index's retriever\n",
    "    try:\n",
    "        retriever = kg_index.as_retriever()\n",
    "        query = \"What is Wollastonite?\"\n",
    "        results = retriever.retrieve(query)\n",
    "\n",
    "        print(f\"\\nQuery: {query}\")\n",
    "        print(\"Results:\")\n",
    "        for i, result in enumerate(results, 1):\n",
    "            print(f\"\\nResult {i}:\")\n",
    "            print(f\"Node ID: {result.node.node_id}\")\n",
    "            print(f\"Score: {result.score}\")\n",
    "            print(f\"Content: {result.node.get_content()[:100]}...\")  # First 100 characters\n",
    "    except Exception as e:\n",
    "        print(f\"Error during retrieval: {str(e)}\")\n",
    "\n",
    "    # 6. Check schema\n",
    "    if kg_index.property_graph_store.supports_structured_queries:\n",
    "        schema = kg_index.property_graph_store.get_schema()\n",
    "        print(\"\\nGraph Schema:\")\n",
    "        print(schema)\n",
    "    else:\n",
    "        print(\"\\nStructured queries not supported by this graph store.\")\n",
    "\n",
    "# Use the function\n",
    "inspect_property_graph_index(kg_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add these imports if not already present\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "# Use the same connection details you used for Neo4jPropertyGraphStore\n",
    "driver = GraphDatabase.driver(neo4j_uri, auth=(neo4j_user, neo4j_password))\n",
    "\n",
    "def run_query(query):\n",
    "    with driver.session(database=\"test\") as session:\n",
    "        result = session.run(query)\n",
    "        return [record for record in result]\n",
    "\n",
    "# Count all nodes\n",
    "node_count = run_query(\"MATCH (n) RETURN count(n) as count\")\n",
    "print(f\"Total nodes: {node_count[0]['count']}\")\n",
    "\n",
    "# Count all relationships\n",
    "rel_count = run_query(\"MATCH ()-->() RETURN count(*) as count\")\n",
    "print(f\"Total relationships: {rel_count[0]['count']}\")\n",
    "\n",
    "# Get node labels\n",
    "labels = run_query(\"CALL db.labels()\")\n",
    "print(\"Node labels:\", [label[0] for label in labels])\n",
    "\n",
    "# Get relationship types\n",
    "rel_types = run_query(\"CALL db.relationshipTypes()\")\n",
    "print(\"Relationship types:\", [rel[0] for rel in rel_types])\n",
    "\n",
    "# Sample of nodes (adjust LIMIT as needed)\n",
    "sample_nodes = run_query(\"MATCH (n) RETURN n LIMIT 5\")\n",
    "print(\"Sample nodes:\")\n",
    "for node in sample_nodes:\n",
    "    print(dict(node['n']))\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   print(\"Creating PropertyGraphIndex...\")\n",
    "   kg_index = PropertyGraphIndex.from_existing(\n",
    "       property_graph_store=graph_store,\n",
    "       llm=Settings.llm,\n",
    "       embed_model=Settings.embed_model,\n",
    "   )\n",
    "   print(\"PropertyGraphIndex created.\")\n",
    "\n",
    "   # Try to access the underlying graph store\n",
    "   if hasattr(kg_index, 'property_graph_store'):\n",
    "       print(\"Graph store accessible from kg_index\")\n",
    "       # Try to get nodes from the graph store\n",
    "       try:\n",
    "           nodes = kg_index.property_graph_store.get()\n",
    "           print(f\"Nodes retrieved from kg_index.graph_store: {len(nodes)}\")\n",
    "       except Exception as e:\n",
    "           print(f\"Error retrieving nodes from kg_index.graph_store: {str(e)}\")\n",
    "   else:\n",
    "       print(\"No direct access to graph_store from kg_index\")\n",
    "\n",
    "   # Try to perform a query\n",
    "   query_engine = kg_index.as_query_engine()\n",
    "   response = query_engine.query(\"What is Wollastonite?\")\n",
    "   print(\"Query response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "bm25_retriever = BM25Retriever(\n",
    "    nodes=nodes,  # These are the same nodes you used for other indexes\n",
    "    similarity_top_k=5,\n",
    "    verbose=True\n",
    ")\n",
    "bm25_retriever.persist(\"bm25_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "bm25_retriever = BM25Retriever.from_persist_dir(\"bm25_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the Retriever\n",
    "Nodes of text will be retrieved from three different spaces:\n",
    "- a vector index based on semantic similarity.\n",
    "- a bm25 index based on keyword matching.\n",
    "- a knowledge graph index based on entities and their relationships.\n",
    "Duplicates are removed and then a Cohere rerank is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Retriever\n",
    "from src.hybrid_graph_retriever import HybridGraphRetriever\n",
    "retriever = HybridGraphRetriever(\n",
    "    vector_index=vector_index,\n",
    "    kg_index=kg_index,\n",
    "    bm25_retriever=bm25_retriever,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Query and Count Tokens\n",
    "We can finally ask our question!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import query_engine\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "# Assuming you've already set up your retriever as shown:\n",
    "# retriever = HybridGraphRetriever(\n",
    "#     vector_index=vector_index,\n",
    "#     kg_index=kg_index,\n",
    "#     bm25_retriever=bm25_retriever,\n",
    "# )\n",
    "\n",
    "# Set up Ollama LLM\n",
    "llm = Ollama(model=\"mistral\")\n",
    "\n",
    "# Create a custom prompt template\n",
    "custom_prompt_template = PromptTemplate(\n",
    "    \"You are an AI assistant specializing in soil analysis and plant nutrition. \"\n",
    "    \"Using only the information provided in the context, answer the question. \"\n",
    "    \"If you cannot answer based solely on the given context, say 'I don't have enough information to answer that question.'\\n\"\n",
    "    \"Context: {context_str}\\n\"\n",
    "    \"Question: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# response = query_engine.query(\"What is the optimal pH for cannabis plants?\")\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core.schema import QueryBundle\n",
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "# Initialize the Ollama LLM\n",
    "# We are directly using the Ollama class in order to get to the tokens.\n",
    "ollama_llm = Ollama(model=\"mistral\")\n",
    "\n",
    "# Create a custom prompt template for Ollama\n",
    "custom_prompt_template = PromptTemplate(\n",
    "    \"You are an AI assistant specializing in soil analysis and plant nutrition. \"\n",
    "    \"Using only the information provided in the context, answer the question. \"\n",
    "    \"If you cannot answer based solely on the given context, say 'I don't have enough information to answer that question.'\\n\"\n",
    "    \"Context: {context_str}\\n\"\n",
    "    \"Question: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "\n",
    "# Function to run a query with the retriever and custom prompt\n",
    "def ask_question(query: str):\n",
    "    # Use the retriever to get relevant context\n",
    "    query_bundle = QueryBundle(query_str=query)\n",
    "    retrieved_nodes = retriever.retrieve(query_bundle)\n",
    "\n",
    "    # Prepare the context string from retrieved nodes\n",
    "    context_str = \"\\n\".join([node.node.text for node in retrieved_nodes])\n",
    "\n",
    "    # Format the prompt using the custom template\n",
    "    formatted_prompt = custom_prompt_template.format(\n",
    "        context_str=context_str,\n",
    "        query_str=query\n",
    "    )\n",
    "\n",
    "    # Use Ollama's chat method with the formatted prompt\n",
    "    from llama_index.core.base.llms.types import ChatMessage, MessageRole\n",
    "\n",
    "    messages = [ChatMessage(role=MessageRole.USER, content=formatted_prompt)]\n",
    "    ollama_response = ollama_llm.chat(messages)\n",
    "\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"answer\": ollama_response.message.content,\n",
    "        \"contexts\": [node.node.text for node in retrieved_nodes],\n",
    "        \"token_info\": {\n",
    "            \"prompt_tokens\": ollama_response.raw.get('prompt_eval_count', 0),\n",
    "            \"completion_tokens\": ollama_response.raw.get('eval_count', 0),\n",
    "            \"total_tokens\": ollama_response.raw.get('prompt_eval_count', 0) + ollama_response.raw.get('eval_count', 0)\n",
    "        },\n",
    "        \"other_info\": {\n",
    "            \"model\": ollama_response.raw.get('model'),\n",
    "            \"total_duration\": ollama_response.raw.get('total_duration'),\n",
    "            \"load_duration\": ollama_response.raw.get('load_duration'),\n",
    "            \"eval_duration\": ollama_response.raw.get('eval_duration')\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "# --->: Set up the local embedding model and LLM\n",
    "# Set embedding model\n",
    "\n",
    "\n",
    "\n",
    "class SimpleFaithfulness:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def evaluate(self, question, answer, context):\n",
    "        prompt = f\"\"\"\n",
    "        Question: {question}\n",
    "        Answer: {answer}\n",
    "        Context: {context}\n",
    "\n",
    "        Evaluate the faithfulness of the answer based on the given context. Consider the following:\n",
    "        1. Does the answer contain information not present in the context?\n",
    "        2. Does the answer contradict any information in the context?\n",
    "        3. Is the answer a fair representation of the information in the context?\n",
    "\n",
    "        Respond with:\n",
    "        1. A score from 0 to 1, where 0 is completely unfaithful and 1 is completely faithful.\n",
    "        2. A brief explanation of your scoring.\n",
    "        3. Any hallucinations or discrepancies found, if any.\n",
    "\n",
    "        Format your response exactly as follows:\n",
    "        Score: [Your score here]\n",
    "        Explanation: [Your explanation here]\n",
    "        Hallucinations: [List any hallucinations or discrepancies, or 'None' if none found]\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            response = ask_question(prompt)\n",
    "            return self._parse_response(response)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during evaluation: {str(e)}\")\n",
    "            return {\"faithfulness\": 0, \"explanation\": \"Error occurred\", \"hallucinations\": \"Unable to evaluate\"}\n",
    "\n",
    "    def _parse_response(self, response):\n",
    "        score_match = re.search(r'Score:\\s*([\\d.]+)', response['answer'])\n",
    "        explanation_match = re.search(r'Explanation:\\s*(.+?)(?:\\n|$)', response['answer'], re.DOTALL)\n",
    "        hallucinations_match = re.search(r'Hallucinations:\\s*(.+?)(?:\\n|$)', response['answer'], re.DOTALL)\n",
    "\n",
    "        score = float(score_match.group(1)) if score_match else 0\n",
    "        explanation = explanation_match.group(1).strip() if explanation_match else \"No explanation provided\"\n",
    "        hallucinations = hallucinations_match.group(1).strip() if hallucinations_match else \"Unable to determine\"\n",
    "\n",
    "        return {\n",
    "            \"faithfulness\": score,\n",
    "            \"explanation\": explanation,\n",
    "            \"hallucinations\": hallucinations\n",
    "        }\n",
    "\n",
    "# Usage example\n",
    "simple_faithfulness = SimpleFaithfulness()\n",
    "# Example usage of simple_faithfulness.evaluate()\n",
    "question = \"What are the benefits of using wollastonite in agriculture?\"\n",
    "# answer = \"Wollastonite is beneficial in agriculture due to its liming capability, silicon content, and calcium content. It can help improve soil pH and provide essential nutrients to plants.\"\n",
    "answer = \"The sky is purple.\"\n",
    "context = \"Wollastonite is a calcium silicate mineral. It is used in agriculture for its liming capability, silicon content, and calcium content. These properties can help improve soil structure and provide nutrients to plants.\"\n",
    "\n",
    "result = simple_faithfulness.evaluate(question, answer, context)\n",
    "print(f\"Faithfulness score: {result['faithfulness']}\")\n",
    "print(f\"Explanation: {result['explanation']}\")\n",
    "print(f\"Hallucinations: {result['hallucinations']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "from ragas.metrics import Faithfulness\n",
    "from datasets import Dataset\n",
    "\n",
    "# Create an instance of OllamaLLM\n",
    "ollama_llm = OllamaLLM(model=\"mistral\")\n",
    "\n",
    "# Create an instance of the Faithfulness metric with your LLM\n",
    "custom_faithfulness = Faithfulness(llm=ollama_llm)\n",
    "\n",
    "def evaluate_single_response(question, answer, contexts):\n",
    "    dataset = Dataset.from_dict({\n",
    "        \"user_input\": question,\n",
    "        \"response\": answer,\n",
    "        \"context\": contexts,\n",
    "        \"ground_truths\": [\"\"]  # Empty string instead of empty list\n",
    "    })\n",
    "\n",
    "    # Use the custom faithfulness metric\n",
    "    result = custom_faithfulness.score(dataset)\n",
    "\n",
    "    return result['faithfulness'][0]\n",
    "\n",
    "# Example usage\n",
    "question = \"What is wollastonite?\"\n",
    "answer = \"Wollastonite is a calcium silicate mineral used in agriculture for its liming capability and silicon content.\"\n",
    "contexts = \"Wollastonite is a calcium silicate mineral. It is used in agriculture for its liming capability, silicon content, and calcium content.\"\n",
    "\n",
    "faithfulness_score = evaluate_single_response(question, answer, contexts)\n",
    "print(f\"Faithfulness score: {faithfulness_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_data = [\n",
    "    {\n",
    "        \"question\": \"What is wollastonite and how does it relate to plant nutrition?\",\n",
    "        \"ground_truth\": \"Wollastonite is a calcium silicate mineral used in agriculture for its liming capability, silicon content, and calcium content. It relates to plant nutrition by providing calcium and silicon, buffering soil pH, and potentially contributing to pest control and powdery mildew suppression.\"\n",
    "    },\n",
    "    # Add more question-answer pairs here\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

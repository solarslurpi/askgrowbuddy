{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Start\n",
    "We start with an already created vector index.\n",
    "Let's get to know how the vector index works by exploring the path an Obsidian note takes from loading through indexing through being retrieved.\n",
    "\n",
    "The document we'll be using, 'Bluelab Pulse Meter Review.md', includes frontmatter as well as transcribed text from a YouTube video.  This note was chosen because it has a moderate length and rich metadata fields.\n",
    "\n",
    "The `load_obsidian_notes` method is used to load the note into a list of LlamaIndex Documents. In this case, there is only one document.  Notice the metadata. There are some interesting fields such as the `description` which came from the frontmatter of the Obsidian note.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n",
    "%pwd  # To verify the current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ingest_service import IngestService\n",
    "from src.doc_stats import DocStats\n",
    "ingest_service = IngestService()\n",
    "# obsidian_notes_path = 'eval/obsidian_notes'\n",
    "obsidian_notes_path = r'G:\\My Drive\\Audios_To_Knowledge\\knowledge\\AskGrowBuddy\\AskGrowBuddy\\Knowledge\\soil_test_knowlege'\n",
    "docs = ingest_service.load_obsidian_notes(obsidian_notes_path)\n",
    "\n",
    "DocStats.print_llama_index_docs_summary_stats(docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have load a document.  Let's break it text nodes.  LlamaIndex's `MarkdownNodeParser` is used to do this.  Besides metadata, Obsidian notes break down text using markdown headers.  By using a `MarkdownNodeParser`, we can use these headers to guide how the text is chunked. The document workflow is using LlamaIndex libraries.  We start with a `LlamaIndexDocument` and now move to a `BaseNode`, or actually a `TextNode`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we print out the docstats, we see that the number of documents (nodes) has increased from 1 to 6.  The content length varies and the nodes all have metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_nodes = ingest_service.chunk_text(docs)\n",
    "DocStats.print_llama_index_docs_summary_stats(text_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Nodes\n",
    "Let's look at the contents of the nodes.  The first node looks like it does not have any interesting content. The only thing in the text is the timestamp-url code block that is used to play the YouTube video.\n",
    "\n",
    "```  timestamp-url\n",
    "https://www.youtube.com/watch?v=KbZDsrs5roI\n",
    "```\n",
    "This surprised me because I expected nodes to start with a Header.  What I have learned so far:\n",
    "- The markdown splitting is aggressive and simple.  It looks for a #. When it sees one, it starts a new node if there is text underneath it. This is how nodes can include very little text.  Some nodes - like the first one in this example - do not contain any semantically relevant information.  In this case, it contains only a timestamp-url codeblock.\n",
    "- The content is interspersed with the timestamp code blocks. These should be removed.\n",
    "- Even after this, the nodes will need to be reviewed and cleaned up.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The End\n",
    "This exploration started with a vector index whose nodes were created with a very agressive Markdown splitter. In addition, the content should be cleaned up. Even after these activities happen,  the nodes will need further review and cleanup.\n",
    "\n",
    "The next step in the exploration is to explore the [node splitting process](https://github.com/solarslurpi/askgrowbuddy/blob/f29dd0ff194471bc546587ea17d603a55b85ff26/eval/eval_markdown_splitter.ipynb).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from node_view import launch_node_viewer\n",
    "# Create and launch the interface\n",
    "launch_node_viewer(text_nodes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's store the nodes in a chromadb vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_service = IngestService()\n",
    "# Create a Chroma collection object of a given name. Metadata, embeddings, text are all added.\n",
    "our_collection = ingest_service.create_collection(docs=docs, collection='test', embedding_model_name='nomic-embed-text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll retrieve the nodes from the vector store and convert them back to LlamaIndex TextNodes to get a closer look at what is in the vector store.  It should be the same view as when the text nodes were first created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from src.ingest_service import IngestService\n",
    "# Grab the vector index\n",
    "ingest_service = IngestService()\n",
    "our_collection = ingest_service.get_collection('test')\n",
    "chroma_vector_store = ChromaVectorStore(chroma_collection=our_collection)\n",
    "# Create a VectorStoreIndex using the ChromaVectorStore\n",
    "vector_index = VectorStoreIndex.from_vector_store(chroma_vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import TextNode\n",
    "def chroma_to_text_nodes(chroma_data):\n",
    "    text_nodes = []\n",
    "    for doc, metadata, node_id in zip(chroma_data['documents'], chroma_data['metadatas'], chroma_data['ids']):\n",
    "        node = TextNode(\n",
    "            text=doc,\n",
    "            metadata=metadata,\n",
    "            id_=node_id\n",
    "        )\n",
    "        text_nodes.append(node)\n",
    "    return text_nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all document IDs\n",
    "doc_ids = our_collection.get()['ids']\n",
    "\n",
    "# Print the number of documents\n",
    "print(f\"Total number of documents: {len(doc_ids)}\")\n",
    "\n",
    "# Retrieve the node\n",
    "node_data = our_collection.get(ids=doc_ids  , include=['documents', 'metadatas'])\n",
    "print(node_data['metadatas'][0])\n",
    "text_nodes = chroma_to_text_nodes(node_data)\n",
    "DocStats.print_llama_index_docs_summary_stats(text_nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and launch the interface\n",
    "iface = create_node_viewer(text_nodes)\n",
    "iface.launch(inline=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's set up the vector index as a retriever and see how it works.  We'll retrieve `similarity_top_k`nodes.  Instead of being type `TextNode`, the nodes are now `NodeWithScore` objects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_index.as_retriever(similarity_top_k=5)\n",
    "response = retriever.retrieve(\"What is the ideal ph for growing Cannabis?\")\n",
    "print(f\"Response type: {type(response[0])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = vector_index.as_query_engine(similarity_top_k=5)\n",
    "response = query_engine.query(\"I am growing Cannabis in Living Soil and my pH is 8.  Is that ok?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(response.source_nodes[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and launch the interface\n",
    "iface = create_node_viewer(response.source_nodes)\n",
    "iface.launch(inline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
